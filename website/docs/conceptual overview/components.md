---
sidebar_position: 3
---

import Components from '../img/components.png'

# Components

Find the basic StatsHouse components in the picture and related descriptions below:

<img src={Components} width="1000"/>



## Agent

An agent receives metric data via UDP in these formats:
* JSON,
* [protobuf](https://protobuf.dev),
* MessagePack,
* TL.

It also supports the RPC TL protocol. The agent **validates** and **interprets** metric data, accumulates data
over a second, **shards** it and sends to aggregators via the RPC TL protocol.
If aggregators are unavailable, the agent stores data on a local disk within the **quota**
and sends it later. There are about 15,000 agents in VK.

### Receiving data: implementation details

Все ошибки приёма пишутся во встроенную метрику __ingestion_status. Если имя метрики найдено, её ID будет записан
в соответствующую колонку метрики, если нет - строчка имени будет записана в строковую колонку.
То же самое произойдёт с не найденным именем тэга.

Счётчики и значения имеют тип float64, и при приёме их значения обрезаются диапазоном значений
[-max(float32)..max(float32)]. Это сделано для того, чтобы при их суммировании и других операциях над ними,
том числе внутри базы данных, никогда не получать значений +-inf. Однако точность float64 сохраняется,
в том числе если исходные значения целые, можно просуммировать довольно много их без потери точности.

Значения-уникумы имеют тип int64, который интерпретируется просто как 64 бита и выбран вместо uint64 потому,
что в некоторых языках нет типа uint64. Эти значения считаются чем-то вроде хэшей и при вычислении кардинальности
составленных из них множеств они просто проверяются на равенство и неравенство. При записи агрегатов (min, max, sum)
эти значения сначала интерпретируются, как int64 и затем конвертируются во float64, так как пишутся в те же колонки,
что агрегаты обычных значений.

Если счётчик указан сам по себе, без массива значений или уников, интерпретируется просто как счётчик.

Значения и уники вместе указывать нельзя, будет записана ошибка приёма.

Если указан массив значений или уников, а счётчик не указан, то число событий считается равным размеру массива.

Если счётчик указан вместе с массивом, то число событий берётся равным счётчику,
а массив считается сэмплом настоящих значений. Так что

```
    ... "counter": 6, "values": [1, 2, 3] ...

```
означает, что число событий 6, а каждого значений по 2.

#### Receiving data via UDP

Мы принимаем пакет в формате MessagePack, Protobuf, JSON, TL, все форматы семантически идентичны и происходит
автоопределение формата по первым байтам пакета.

Пакет является объектом, содержащим массив метрик

```
    {"metrics":[ ... ]}

```
Каждый элемент массива является объектом с полями:
```
    {
     "name":"rpc_call_latency",  // имя метрики, обязательно
     "tags":{"protocol": "tcp"}, // тэги
     "ts": 1630000000,           // время событий, отсутствие или 0 означает 'сейчас'
     "counter": 6,               // счётчик события (событий)
     "value": [1, 2.0, -3.0],    // значения, если есть (нельзя вместо с уникумами)
     "unique": [15, 18, -60]     // уникумы, если есть (нельзя вместе со значениями)
    }
```
Например, можно отправить вот такой пакет
```
    {"metrics":[
    {"name":"rpc_call_latency",
     "tags":{"protocol": "tcp"},
     "value": [15, 18, 60]},
    {"name": "rpc_call_errors",
     "tags":{"protocol": "udp","error_code": "-3000"},
     "counter": 5}
    ]}
    {"metrics":[
    {"name": "external_landings",
     "tags":{"country": "ru","sex": "m","skey": "lenta.ru"},
     "counter": 1}
    ]}
```

Определения для Protobuf:
```
    message Metric {
      string              name    = 1;
      map<string, string> tags    = 2;
      double              counter = 3;
      uint32              ts      = 4;  // UNIX seconds UTC
      repeated double     value   = 5;
      repeated int64      unique  = 6;
    }
    
    message MetricBatch {
      repeated Metric metrics = 13337;  // to autodetect packet format by first bytes
    }
```
В случае TL, тело пакета должно быть Boxed-сериализацией объекта statshouse.addMetricsBatch (определение ниже)

В случае JSON, первый символ должен быть скобкой `{`, иначе автоопределение не сработает.

В случае Protobuf, по той же причине нельзя добавлять в объект MetricBatch никаких дополнительных полей.

##### UDP socket buffer overflow

Without a client library, you can create a socket, prepare a JSON file, and send your formatted data.
This sounds simple, but only if you have not so much data.

StatsHouse uses [UDP](https://en.wikipedia.org/wiki/User_Datagram_Protocol).
If you send a datagram per event, and there are too many of them,
there is a risk of dropping datagrams, and no one will notice it.

If you do not use the client library, the non-aggregated data will reach StatsHouse
[agent](#agent), and the agent will aggregate them anyway.

#### Receiving data via TL RPC

```
    statshouse.metric#3325d884 fields_mask:#
      name:    string
      tags:    (dictionary string)
      counter: fields_mask.0?double
      ts:      fields_mask.4?#               // UNIX timestamp UTC
      value:   fields_mask.1?(vector double)
      unique:  fields_mask.2?(vector long)
    
    = statshouse.Metric;
    
    @write statshouse.addMetricsBatch#56580239 fields_mask:#
      metrics:(vector statshouse.metric)
    = True;

```
Ошибки приёма возвращаются, как TL ошибки. Мы не специфицируем коды ошибок, так как не предполагаем никакой логики
в клиентах при получении ошибок, только запись в лог и последующий анализ вручную.

#### Receiving data via unix datagram socket or TCP

Если можно делать неблокирующую отправку в unix datagram socket, то можно использовать их вместо UDP для того,
чтобы отследить потерю пакетов со стороны отправителя.

Либо клиенты, например PHP, могут с той же целью пользоваться non-blocking TCP/unix socket для того,
чтобы не блокироваться, когда буфер сокета переполняется (хвостик пакета, не влезший целиком запоминается,
и будет отправлен по мере освобождения буфера. Новые же пакеты будут выбрасываться целиком,
а их счётчик будет увеличиваться).

## Aggregator

It aggregates per-second metric data from all the agents and inserts the resulting aggregation into a ClickHouse
database. There are as many aggregators as there are ClickHouse shards with replicas. VK has eight shards each
having three replicas, i.e., there are 24 aggregators. Each aggregator inserts data to its local database replica
deployed on the same machine.

### An agent-aggregator interaction

Взаимодействие между агентом и агрегатором: детали

Аггрегатор имеет 2 логических точки входа для данных, одна для отправки “актуальных” данных, другая -
для отправки “исторических” данных, то есть тех, которые не удалось отправить сразу после создания.

Аггерагтор всегда приоритизирует вставку актуальных данных, поэтому после сбоя, когда сначала данные долго
не могли вставиться, но потом нормальный ход вставки возобновился, актуальные данные начинают вставляться немедленно,
а вот накопившийся за время сбоя объём исторических данных будет вставлен по возможности, настолько быстро,
насколько это не мешает вставке актуальных данных.

Аггрегатор позволяет вставлять актуальные данные за последние 5 секунд (короткое окно, настраивается),
если агент не успел, ему отправляется ответ “присылай данные, как исторические”. Для каждой актуальной секунды
агрегатор хранит контейнер со статистикой, куда и агрегируется данные клиентов, как только наступает следующая секунда,
данные секунды, выходящей из короткого окна вставляются в clickhouse, а агенты получают ответ с результатом вставки.
Также короткое окно распространяется на две секунды в будущее, чтобы нормально работали клиенты,
у которых часы немного идут вперёд.

Аггрегатор позволяет вставлять исторические данные за последние 2 суток (длинное окно, настраивается).
Если приходят более старые данные, пишется специальная метастатистика, данные выбрасываются,
на агент отправляется ответ ОК.

Поскольку агрегация данных между машинами очень важна, каждый агент делает запрос на вставку нескольких десятки
исторических секунд, начиная от самой старой, агрегатор принимает все эти запросы, затем выбирает самую старую секунду,
агрегирует, вставляет и присылает ответ, затем снова выбирает самую старую секунду, и так далее. Такой алгоритм
приводит к тому, что отставшие сильнее агенты “догоняют” менее отставших, таким образом создаётся тенденция
агрегировать и вставлять каждую историческую секунду максимально одновременно, один раз.

Поскольку агрегатор должен лимитировать объём вставленных в секунду данных, а часть данных может вставляться позже,
как исторические, агрегатор учитывает, сколько агентов внесли вклад в секунду, и устанавливает лимит пропорционально
(все агенты каждую секунду присылают метаданные, даже если никаких пользовательских событий не было).
Поэтому, если актуальные данные за какую-то секунду прислали 80% агентов, то для их вставки будет использовано
80% канала, затем если исторические данные прислали ещё 15% агентов, для их вставки будет выделено ещё 15% канала, и т.д.
Этот алгоритм приводил к слишком большому сэмплированию, когда агентов очень мало, поэтому в этом случае к лимиту
делается небольшая поправка в сторону увеличения.

Каждый шард-реплика агента при старте выбирает поправку к часам от -1 до 0 секунд в зависимости от номера реплики,
для того, чтобы агенты не присылали данные лавиной в момент переключения секунды, приводя к большому
количеству потерянных пакетов.

Агент сохраняет данные на диск в случае получения ошибки от агрегатора или его недоступности и хранит там либо
до достижения лимита в байтах, либо до истечении длинного окна в двое суток, когда известно,
что эти данные агрегатор всё равно не примет.

В некоторых случаях, когда доступ к диску нежелаелен или невозможен, агенты может быть запущен
с пустым аргументом `--cache-dir`, тогда диск не будет использоваться, а хранение исторических данных будет в памяти,
и всего на несколько минут отсутствия подключения к агрегаторам.

### Preventing double inserts and handling aggregator's shutdowns

Предотвращение двойной вставки и работа при отказе агрегатора

Если агент получает от агрегатора ошибку, и после этого отправляет те же самые данные другому агрегатору,
находящемуся на другой машине, то для дедупликации нужна система консенсуса. Мы решили, что сложность такой системы
слишком высока, поэтому у нас возможна ситуация, когда и основной и запасной агрегатор вставят данные
от одного агента за одну секунду. Мы решили, что это происходит нечасто, и вместо того, чтобы предотвратить
двойную вставку, мы контролируем её специальной метаметрикой “количество агентов, приславших данные в эту секунду”.
Для того чтобы эта метаметрика была стабильной при нормальной работе, агенты присылают данные каждую секунду,
даже если никакой пользовательской статистики в эту секунду не было. Также, если агент обнаруживает,
что часы сдвинулись вперёд больше, чем на секунду (например, машина затупила или уснула), он присылает разницу
в специальном поле, чтобы агрегаторы могли учесть это в специальной метаметрике.

При отказе агрегатора, данные, предназначеныые ему, отправляются агентами на один из двух агрегаторов-реплик,
распределяясь между ними по номеру секунды - чётные секунды идут на одну из оставшихся, нечётные на другую,
так что в среднем нагрузка на каждый вырастает на 50%. Это одна из причин того, что мы поддерживаем запись
строго на 3 реплики clickhouse.

## Database

The [ClickHouse](https://clickhouse.com) database stores aggregated metric data.

### ClickHouse table structure

Вся статистика всех типов для всех метрик исходно сохраняется в одну таблицу clickhouse,
один ряд соответствует одному агрегату. Примерное определение таблицы такое.

```
    CREATE TABLE statshouse2_value_1s (
        `time`           DateTime,
        `metric`         Int32,
        `tag0`           Int32,
        `tag1`           Int32,
    ...
        `tag15`          Int32,
        `stag`           String,
        `count`          SimpleAggregateFunction(sum, Float64),
        `min`            SimpleAggregateFunction(min, Float64),
        `max`            SimpleAggregateFunction(max, Float64),
        `sum`            SimpleAggregateFunction(sum, Float64),
        `max_host`       AggregateFunction(argMax, Int32, Float32), 
        `percentiles`    AggregateFunction(quantilesTDigest(0.5), Float32),
        `uniq_state`     AggregateFunction(uniq, Int64)
    ) ENGINE = *MergeTree
    PARTITION BY toDate(time) ORDER BY (metric, time,tag0,tag1, ...,tag15, stag);
```

Если метрик не является перцентилем или счётчиком уникумов, то значение в соответствующей колонке будет пустым.
Также, для обычного счётчика все колонки, кроме count будут нулевыми/пустыми. Аналогично колонка строкового тэга
будет непустой только если использован топ строк.

Данные из этой таблицы агрегируются за некоторые временные интервалы (например, 60 секунд, 3600 секунд) и сохраняются
в идентичные таблицы, но с другим именем, для поддержки быстрой выборки за временные интервалы,
значительно превышающие секунду.

Данные шардируются между шардами clickhouse по хэшу от (metric, key0, … , key15), поэтому части данных метрики, с
оответствующие, например, метке `"protocol":"tcp"` обычно оказываются на разных шардах из-за разных значений других
тэгов (на всех шардах, если комбинаций тэгов много), и для получения полной картины нужно всегда делать
Distributed Query ко всем шардам. Это так, потому что набор тэгов имеет ограниченную кардинальность, и при её
достижении объём данных при агрегации перестаёт расти, поэтому если бы каждый шард хранил “сэмпл” всей статистики,
то при достаточном объёме данных, фактически каждому шарду пришлось бы хранить число рядов равное кардинальности
статистики, а не долю, пропорциональную числу шардов.

Буферные таблицы не используются, так как обычно каждый агрегатор делает 1 вставку в секунду. Вставка делается в
incoming-таблицу, а копирование оттуда с помощью материализованного представления с фильтрацией значений по time в
пределах окна приёма данных (двое суток), это защита от ошибочной вставки мусорных данных, которая приведёт к тому,
что при запросах clickhouse придётся читать данные из всех партов, а не 1-2.

Количество реплик каждого шарда должно быть больше или ровно 3, первые три будут использоваться для вставки
агрегаторами, остальные считаются readonly-репликами и могут разворачиваться для масштабирования нагрузки чтений.
Количество шардов может быть каким угодно (мы используем 8, в планах увеличить до 16). Для предотвращения неправильной
конфигурации агентов и разного шардирования, которое бы привело к взрывному росту объёма данных из-за плохой агрегации,
агенты присылают номер шарда-реплики с каждым пакетом данных а агрегатор, и если агрегатор видит, что данные
предназначаются не ему, отвечает ошибкой. Этот же номер шарда-реплики используется ingress proxy для того, чтобы
направить данные на правильный агрегатор.

## Data access service

A data access service allows StatsHouse to send efficient queries to the database using a thin API.
The service caches data to minimize a database load. We limit retrieving data directly from ClickHouse
as much as possible, since ineffective queries can negatively impact the ClickHouse cluster.

## User interface

A user interface retrieves data from `statshouse-api` and displays metric data in a graph view.

## Ingress proxy

An ingress proxy receives data from the agents that live outside the protected perimeter
(i.e., outside the data center) and sends it to the aggregators.

Поскольку агенты и агрегаторы используют протокол TL RPC с ключом шифрования датацентра, агенты вне датацентра не могут
напрямую присоединяться к агрегаторам, так как это бы потребовало копирования/раскрытия ключа датацентра
на внешние площадки.

Поэтому ingress proxy имеет отдельный набор ключей шифрования для подключения извне. Любой ключ может отзываться
простым удалением из конфигурации ingress proxy.

Ingress proxy не имеет состояния, и для уменьшения вероятности атаки проксирует только подмножество типов запросов
TL RPC, используемых агрегаторами.

Ingress proxy должно быть ровно 3 штуки, каждая проксирует в соответсвующую реплику каждого шарда.
Поэтому отказ или остановка для обслуживания одной из ingress proxy эквивалентен отказу одной из реплик каждого шарда,
что не мешает нормальной работе системы.

### Cryptokeys

Статсхаус использует VK RPC с (опциональным) шифрованием для общения всех компонентов. В VK RPC криптоключ является
и паролем для входа,
и секретом для вывода эфемерных ключей соединения.

Для того, чтобы установить связь, клиент должен использовать при установлении соединения один из ключей,
известных серверу.

Центральным компонентам системы являются аггрегаторы, они при запуске получают единственный "главный" криптоключ
датацентра.

Все агенты для подключения к аггрегаторам должны получить адреса первого шарда аггрегаторов в параметре `-agg-addr=X`,
а также "главный" криптоключ датацентра в параметре `-aes-pwd-file=X`.

Однако это безопасно только внутри защищённого периметра. Если есть необходимость подключаться извне,
используется ingress proxy, установленные как раз на границе периметра.

Ингресс прокси стоит на границе, и у неё есть 2 половинки - RPC сервер для того, чтобы агенты подключались снаружи,
и RPC клиент для подключения самой прокси к аггрегаторам внутри.

Ингресс прокси обязаны знать свои внешние адреса, по которым к ним будут подключаться агенты, эти адреса даются
в аргументе `-ingress-external-addr`.
Для управления тем, на каких интерфейсах могут подключаться агенты используется аргумент (`-ingress-addr=X`,
обычно `:8128` что эквивалентно `0.0.0.0:8128`, либо адрес подсети сетевого адаптера, чтобы ограничить подключение
только через него).
Порт в нём должен совпадать с портами в `-ingress-external-addr`.

По этим адресам внешняя половинка ингресс прокси должна быть доступна для агентов.

В настройках ingress proxy указывается как внутренний криптоключ для отправки данных аггрегаторам `-aes-pwd-file=X`,
так и множество внешних ключей для агентов,
установленных на удалённых площадках `-ingress-pwd-dir=X`, содержимое каждого файла это криптоключ,
имя файла игнорируется, и считается комментарием. Длина ключей произвольна, но не меньше 4 байтов, первые 4 байта
используются для идентификации ключей и не могут совпадать.
При изменении содержимого папки нужно перезапустить ingress proxy, механизма слежения за папкой нет,
так как набор ключей меняется крайне редко.

Соответственно каждый агент получает в `-aes-pwd-file=X` один из ключей указанных у прокси в папке `-ingress-pwd-dir=X`.

### A proxy behind the proxy

Тройка Ingress proxy имитирует агрегаторы, так что можно установить ещё один уровень ingress proxy,
который будет в качестве агрегаторов использовать предыдущую прокси.

Тогда для следующей ingress proxy в аргументе `-agg-addr=X` даётся тройка адресов из `-ingress-external-addr=X`
предыдущей прокси, а в качестве криптоключа в `-aes-pwd-file=X` один из криптоключей в папке `-ingress-pwd-dir=X`
предыдущей прокси.

### Kubernetes-related questions

При шифровании в VK RPC для вывода эфемерных ключей используются
remote- и local- IP-адреса соединений, как их видят клиент и сервер, так что трюки типа перекладывания пакетов
между адаптерами с помощью средств firewall приведут к невозможности установления соединений.
Если соединяются, например, kubernetes-подобные компоненты, то придётся создать
виртуальные сетевые адаптеры и подключить их друг к другу средствами linux network namespaces.

Однако крайне не рекомендуется разворачивать агенты и ingress proxy внутри подов. Лучше устанавливать агенты
на контейнеровозы, прокидывая в поды порт 13337.

Эта рекомендация потому, что статсхаусу не нравится, когда количество агентов флуктуирует.
Агенты отчитываются на агрегаторы строго каждую секунду,
и неизменное число агентов являесят важным индикатором того, что все агенты имеют связь до аггрегаторов.
Остановка подов не должна приводить к уменьшению этого числа, так как сработает главный алерт.

## Metadata

A metadata system stores a list of metrics and their settings.
It also holds a global `string`↔`int32` mapping:
StatsHouse maps tag values (which are string values) to `int32` values for higher efficiency.
This huge map is common for all metrics. The metadata system prevents it from being overloaded.

Свойства каждой метрики хранятся в специальном сервисе метаданных, развёрнутом обычно на машинах первого
шарда агрегаторов. Поскольку нагрузка на сервис данных невелика, нет смысла разворачивать этот сервис на отдельных
машинах.

Для каждой метрики хранится её тип (для правильного отображения), имя метрики, имена и способ интерпретации тэгов.

Метрики создаются через UI, автоматического создания метрик не происходит. Это важно, так как
все компоненты предполагают, что разных метрик немного (максимум десятки тысяч)
и не имеют защиты от неконтролируемого роста числа разных метрик.

При миграции с существующего решения можно включить режим “автосоздания”, который создаст все реально используемые
метрики, затем мы рекомендуем отключит автосоздание, так как иначе может произойти создание слишком большого числа
метрик, например если кто-то запишет в имя метрики rand.

Аггрегаторы находятся в постоянном TL RPC long poll к сервису метаданных на изменение метаданных метрик,
а агенты аналогично в long poll к агрегаторам, поэтому изменения свойств метрики уже через секунду
отражаются на всех агентах.

Удалять метрики нельзя, так как нет способа сделать это эффективно в базе данных ClickHouse.
Поэтому используется скрытие метрики установкой флага visible (это действие обратимо).
При этом статистика по этой метрике перестаёт записываться в базу данных.

### Mapping string values

Сервис метаданных хранит взаимно однозначные отображение в своей базе.

```
    'iphone' <=> 12
    'null' <=> 26
```

Flood-лимиты на создание хранятся в той же базе данных.

Аггрегаторы работают с сервисом данных напрямую, и кэшируют отображения в памяти и файлах (на месяц),
агенты работают с отображениями через агрегаторы, и также кэшируют отображения в памяти и файлах (тоже на месяц).
Также агенты при старте с чистого листа используют специальный boostrap запрос из примерно 100000 самых
распространённых отображений, это нужно так как при разворачивании на 10000 машинах пришлось бы скачать с агрегаторов
примерно миллиард значений по-одному, что заняло бы долгое время, в течение которого метрики бы не могли писаться.
